{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Task Fine-tuning of Qwen/Qwen3-14B with LoRA in Colab\n",
    "\n",
    "This notebook demonstrates how to fine-tune the Qwen/Qwen3-14B model for multiple tasks using LoRA (Low-Rank Adaptation) and a custom data collator and trainer. \n",
    "\n",
    "**Note:** Qwen/Qwen3-14B is a large model. You will likely need a Colab Pro subscription with access to a high-RAM GPU (e.g., A100, V100) to run this notebook successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Setup Environment\n",
    "\n",
    "Install the necessary libraries. Restart the runtime after installation if prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.10/site-packages (3.5.1)\n",
      "Requirement already satisfied: peft in ./.venv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: trl in ./.venv/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: bitsandbytes in ./.venv/lib/python3.10/site-packages (0.45.5)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: safetensors in ./.venv/lib/python3.10/site-packages (0.5.3)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting tensorboard\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/12/4f70e8e2ba0dbe72ea978429d8530b0333f0ed2140cc571a48802878ef99/tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./.venv/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.venv/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./.venv/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.10/site-packages (from datasets) (3.11.18)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.10/site-packages (from trl) (14.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.10/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.10/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.10/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.10/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.10/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.10/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.10/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.3.0->torch) (59.6.0)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f6/d4/349f7f4bd5ea92dab34f5bb0fe31775ef6c311427a14d5a5b31ecb442341/absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5d/b7/7e7b7bb6bb18baf156fd4f2f5b254150dcdd6cbf0def1ee427a2fb2bfc4d/grpcio-1.71.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/51/3f/afe76f8e2246ffbc867440cbcf90525264df0e658f8a5ca1f872b3f6192a/markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/28/50/1925de813499546bc8ab3ae857e3ec84efe7d2f19b34529d0c7c3d02d11d/protobuf-6.30.2-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Requirement already satisfied: six>1.9 in ./.venv/lib/python3.10/site-packages (from tensorboard) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/52/24/ab44c871b0f07f491e5d2ad12c9bd7358e527510618cb1b803a88e986db1/werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.venv/lib/python3.10/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.10/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.10/site-packages (from rich->trl) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, protobuf, markdown, grpcio, absl-py, tensorboard\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [tensorboard]\u001b[0m [tensorboard]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.2.2 grpcio-1.71.0 markdown-3.8 protobuf-6.30.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets peft trl bitsandbytes accelerate safetensors torch tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up the parameters for the fine-tuning process. These were originally passed as command-line arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Subset: 8111\n",
      "  Dataset file suffix: 6\n",
      "  Dataset file path: dataset/trainset6.json\n",
      "  Weight Beta (explanation loss): 1.0\n",
      "  Weight Gamma (hunk loss): 0.0\n",
      "  Max length: 2048\n",
      "  Model name: Qwen/Qwen3-14B\n",
      "  Output directory: 8111/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# --- Configuration values based on the command: ---\n",
    "# python3 finetune_codellama_multitask_const.py 8111 6 1 0\n",
    "\n",
    "SUBSET_VAL = \"8111\"  # Corresponds to sys.argv[1]\n",
    "DATASET_FILE_SUFFIX = \"6\"  # Corresponds to sys.argv[2]\n",
    "WEIGHT_BETA = 1.0  # Corresponds to sys.argv[3], for explanation task loss\n",
    "WEIGHT_GAMMA = 0.0  # Corresponds to sys.argv[4], for hunk task loss\n",
    "\n",
    "MAX_LEN = 2048  # Max sequence length for processing\n",
    "MODEL_NAME = \"Qwen/Qwen3-14B\" # Model from Hugging Face Hub\n",
    "\n",
    "# --- Dataset Path --- \n",
    "# IMPORTANT: Update this path if your dataset is located elsewhere (e.g., Google Drive)\n",
    "DATASET_FILE_PATH = f\"dataset/trainset{DATASET_FILE_SUFFIX}.json\"\n",
    "# Example for Google Drive: DATASET_FILE_PATH = f\"/content/drive/MyDrive/datasets/trainset{DATASET_FILE_SUFFIX}.json\"\n",
    "\n",
    "OUTPUT_DIR = f\"{SUBSET_VAL}/\" # Directory to save checkpoints and final model\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration:\\n\"\n",
    "      f\"  Subset: {SUBSET_VAL}\\n\"\n",
    "      f\"  Dataset file suffix: {DATASET_FILE_SUFFIX}\\n\"\n",
    "      f\"  Dataset file path: {DATASET_FILE_PATH}\\n\"\n",
    "      f\"  Weight Beta (explanation loss): {WEIGHT_BETA}\\n\"\n",
    "      f\"  Weight Gamma (hunk loss): {WEIGHT_GAMMA}\\n\"\n",
    "      f\"  Max length: {MAX_LEN}\\n\"\n",
    "      f\"  Model name: {MODEL_NAME}\\n\"\n",
    "      f\"  Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from safetensors.torch import save_model\n",
    "import pandas as pd\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "logging.set_verbosity_info() # Set logging verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset\n",
    "\n",
    "**Important:** You need to upload your dataset file (e.g., `trainset6.json`) to your Colab environment. \n",
    "1. Create a directory named `dataset` in your Colab root.\n",
    "2. Upload your `trainset<SUFFIX>.json` file into this `dataset` directory.\n",
    "Alternatively, if your dataset is on Google Drive, mount your drive and update `DATASET_FILE_PATH` in the configuration cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1535\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Example of creating the directory (run this once if needed)\n",
    "# os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Check if the dataset file exists before loading\n",
    "if not os.path.exists(DATASET_FILE_PATH):\n",
    "    print(f\"ERROR: Dataset file not found at {DATASET_FILE_PATH}\")\n",
    "    print(\"Please upload your dataset to the specified path or update the DATASET_FILE_PATH variable.\")\n",
    "    # You might want to stop execution here if the file is not found\n",
    "    # raise FileNotFoundError(f\"Dataset file not found at {DATASET_FILE_PATH}\")\n",
    "else:\n",
    "    full_dataset = load_dataset(\"json\", data_files=DATASET_FILE_PATH, split=\"train\")\n",
    "    print(f\"Dataset loaded successfully: {full_dataset}\")\n",
    "    \n",
    "    # Optionally, split into train and eval sets if needed for your workflow\n",
    "    # if full_dataset.num_rows > 1: # Ensure there's enough data to split\n",
    "    #     train_test_split = full_dataset.train_test_split(test_size=0.1) # Adjust test_size as needed\n",
    "    #     train_dataset = train_test_split['train']\n",
    "    #     eval_dataset = train_test_split['test']\n",
    "    #     print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    #     print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "    # else:\n",
    "    #     train_dataset = full_dataset\n",
    "    #     eval_dataset = None # Or a small subset if you still want evaluation\n",
    "    #     print(f\"Using full dataset as train_dataset. No evaluation dataset created due to small size.\")\n",
    "    train_dataset = full_dataset # Using the full dataset for training as per original script\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model and Tokenizer Initialization\n",
    "\n",
    "Load the base model with 4-bit quantization and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 17408,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 40,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/model.safetensors.index.json\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:27<00:00,  3.41s/it]\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-14B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/vocab.json\n",
      "loading file merges.txt from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/merges.txt\n",
      "loading file tokenizer.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EOS token: <|im_end|>, EOS token ID: 151645\n",
      "Using SPLIT_TOKEN_ID: 151645 (<|im_end|>) for splitting tasks.\n",
      "Using END_OF_CHUNK_TOKEN_ID: 2 ('#') to mark end of input chunks.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    quantization_config=bnb_config, \n",
    "    trust_remote_code=True,\n",
    "    device_map={\"\":torch.cuda.current_device()} # Ensure model is on GPU\n",
    ")\n",
    "base_model.config.use_cache = False # Recommended for training\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(f\"EOS token: {tokenizer.eos_token}, EOS token ID: {tokenizer.eos_token_id}\")\n",
    "# The original script uses token ID 151645 for splitting, which is <|im_start|> for Qwen\n",
    "# tokenizer.decode(151645) should give '<|im_start|>'\n",
    "SPLIT_TOKEN_ID = 151645 \n",
    "print(f\"Using SPLIT_TOKEN_ID: {SPLIT_TOKEN_ID} ({tokenizer.decode(SPLIT_TOKEN_ID)}) for splitting tasks.\")\n",
    "END_OF_CHUNK_TOKEN_ID = 2 # Original script appends [2] which is often newline '\\n'\n",
    "print(f\"Using END_OF_CHUNK_TOKEN_ID: {END_OF_CHUNK_TOKEN_ID} ({repr(tokenizer.decode(END_OF_CHUNK_TOKEN_ID))}) to mark end of input chunks.\")\n",
    "\n",
    "tokenizer.padding_side = \"right\"  # Fix for potential overflow issues with fp16 training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Data Collator\n",
    "\n",
    "This data collator splits each input example into three parts based on a special separator token: prediction, explanation, and hunks. It assumes your input data is formatted with this separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPrefixDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        pred_features, expl_features, hunk_features_list = [], [], []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Assuming 'text' field is tokenized into 'input_ids' and 'attention_mask'\n",
    "            # If your dataset loading doesn't do this automatically, you might need to tokenize here or earlier\n",
    "            if 'input_ids' not in feature:\n",
    "                 # Example tokenization (adapt if needed based on how your dataset is structured)\n",
    "                 tokenized = tokenizer(feature['text'], truncation=True, max_length=MAX_LEN + 100, padding=False) # Pad later in collator\n",
    "                 input_ids = tokenized['input_ids']\n",
    "                 attention_mask = tokenized['attention_mask']\n",
    "            else:\n",
    "                 input_ids = feature['input_ids']\n",
    "                 attention_mask = feature['attention_mask']\n",
    "            # print(f\"Original input_ids length: {len(input_ids)}\")\n",
    "\n",
    "            # Find indices of the split token (e.g., <|im_end|>, ID 151645 for Qwen)\n",
    "            split_indices = [i for i, x in enumerate(input_ids) if x == SPLIT_TOKEN_ID]\n",
    "\n",
    "            # Ensure at least three split points for prefix, task1_content, task2_content, task3_content ...\n",
    "            # Format expected: <prefix><SPLIT_TOKEN_ID><pred_content><SPLIT_TOKEN_ID><expl_content><SPLIT_TOKEN_ID><hunk1_content><SPLIT_TOKEN_ID>...<hunkN_content>\n",
    "            if len(split_indices) < 3: # Needs prefix, pred, expl separators\n",
    "                print(f\"Warning: Not enough split points ({len(split_indices)}) found in an example. Expected at least 3. Skipping example.\")\n",
    "                # print(f\"Problematic input_ids: {input_ids}\")\n",
    "                # print(f\"Decoded: {tokenizer.decode(input_ids)}\")\n",
    "                continue\n",
    "            \n",
    "            prefix_ids = input_ids[:split_indices[0]]\n",
    "            prefix_mask = attention_mask[:split_indices[0]]\n",
    "            \n",
    "            # Prediction task: prefix + prediction content\n",
    "            pred_content_ids = input_ids[split_indices[0]+1:split_indices[1]]\n",
    "            pred_input_ids = (prefix_ids + pred_content_ids)[:MAX_LEN-1] + [END_OF_CHUNK_TOKEN_ID]\n",
    "            pred_input_mask = (prefix_mask + attention_mask[split_indices[0]+1:split_indices[1]])[:MAX_LEN-1] + [1]\n",
    "            pred_features.append({\n",
    "                'input_ids': pred_input_ids,\n",
    "                'attention_mask': pred_input_mask\n",
    "            })\n",
    "            \n",
    "            # Explanation task: prefix + explanation content\n",
    "            expl_content_ids = input_ids[split_indices[1]+1:split_indices[2]]\n",
    "            expl_input_ids = (prefix_ids + expl_content_ids)[:MAX_LEN-1] + [END_OF_CHUNK_TOKEN_ID]\n",
    "            expl_input_mask = (prefix_mask + attention_mask[split_indices[1]+1:split_indices[2]])[:MAX_LEN-1] + [1]\n",
    "            expl_features.append({\n",
    "                'input_ids': expl_input_ids,\n",
    "                'attention_mask': expl_input_mask\n",
    "            })\n",
    "            \n",
    "            # Hunk tasks: prefix + hunk_i content\n",
    "            current_hunk_batch = []\n",
    "            # Iterate through hunk separators until the end\n",
    "            for i in range(2, len(split_indices)):\n",
    "                 start_idx = split_indices[i] + 1\n",
    "                 end_idx = split_indices[i+1] if (i + 1) < len(split_indices) else len(input_ids) # Go to end if last hunk\n",
    "                 hunk_content_ids = input_ids[start_idx:end_idx]\n",
    "                 if not hunk_content_ids: # Skip if a hunk segment is empty\n",
    "                     # print(f\"Warning: Empty hunk segment detected at index {i}. Split indices: {split_indices}\")\n",
    "                     continue\n",
    "\n",
    "                 hunk_input_ids = (prefix_ids + hunk_content_ids)[:MAX_LEN-1] + [END_OF_CHUNK_TOKEN_ID]\n",
    "                 hunk_input_mask = (prefix_mask + attention_mask[start_idx:end_idx])[:MAX_LEN-1] + [1]\n",
    "                 current_hunk_batch.append({\n",
    "                     'input_ids': hunk_input_ids,\n",
    "                     'attention_mask': hunk_input_mask\n",
    "                 })\n",
    "            if current_hunk_batch: # only add if hunks were processed\n",
    "                 hunk_features_list.append(current_hunk_batch)\n",
    "            elif WEIGHT_GAMMA != 0.0: # If gamma is non-zero, we expect hunks\n",
    "                print(f\"Warning: No hunks processed for an example, but WEIGHT_GAMMA is {WEIGHT_GAMMA}. Split indices: {split_indices}\")\n",
    "        \n",
    "        if not pred_features or not expl_features:\n",
    "             # This can happen if all examples in a batch are skipped or invalid\n",
    "             print(\"Warning: No valid prediction or explanation features to collate after processing. Batch might be empty or all examples were invalid.\")\n",
    "             # Return empty/dummy batch structure expected by the trainer\n",
    "             dummy_batch = super().__call__([tokenizer(\"\", return_tensors=\"pt\")], return_tensors=return_tensors) # Create a dummy batch using base class\n",
    "             # Need labels for loss computation, clone input_ids for Causal LM\n",
    "             if 'input_ids' in dummy_batch: dummy_batch['labels'] = dummy_batch['input_ids'].clone()\n",
    "\n",
    "             return {\n",
    "                 'pred': dummy_batch,\n",
    "                 'expl': dummy_batch,\n",
    "                 'hunk': [], # Hunks expect a list of batches\n",
    "             }\n",
    "\n",
    "        if WEIGHT_GAMMA != 0.0 and not hunk_features_list:\n",
    "             # If gamma is non-zero but no hunks were found in the *entire batch*, issue a warning.\n",
    "             # We still proceed with pred/expl.\n",
    "             print(f\"Warning: WEIGHT_GAMMA is {WEIGHT_GAMMA}, but no valid hunk features found in the entire batch.\")\n",
    "\n",
    "        # print(f\"Collating {len(pred_features)} pred, {len(expl_features)} expl, {len(hunk_features_list)} hunk groups.\")\n",
    "        \n",
    "        # Use base class's __call__ to handle padding and tensor conversion for each task type\n",
    "        collated_pred_features = super().__call__(pred_features, return_tensors)\n",
    "        collated_expl_features = super().__call__(expl_features, return_tensors)\n",
    "        \n",
    "        collated_hunk_features_batches = []\n",
    "        if WEIGHT_GAMMA != 0.0:\n",
    "            for hunk_batch in hunk_features_list: # each item is a list of hunk dicts for ONE original example\n",
    "                if hunk_batch: # if there are actual hunks for this example\n",
    "                     # Collate the hunks belonging to the *same original example* together\n",
    "                     collated_hunks_for_example = super().__call__(hunk_batch, return_tensors)\n",
    "                     collated_hunk_features_batches.append(collated_hunks_for_example)\n",
    "                # else: # No hunks for this original example, should have been filtered earlier or gamma is 0\n",
    "                    # print(\"Warning: An empty hunk_batch encountered during collation.\")\n",
    "        \n",
    "        # print('Collation done for one batch.')\n",
    "        # Add labels for standard Causal LM training if not already present\n",
    "        if 'labels' not in collated_pred_features and 'input_ids' in collated_pred_features:\n",
    "            collated_pred_features['labels'] = collated_pred_features['input_ids'].clone()\n",
    "        if 'labels' not in collated_expl_features and 'input_ids' in collated_expl_features:\n",
    "            collated_expl_features['labels'] = collated_expl_features['input_ids'].clone()\n",
    "        for hunk_batch in collated_hunk_features_batches:\n",
    "             if 'labels' not in hunk_batch and 'input_ids' in hunk_batch:\n",
    "                  hunk_batch['labels'] = hunk_batch['input_ids'].clone()\n",
    "\n",
    "        return {\n",
    "            'pred': collated_pred_features,\n",
    "            'expl': collated_expl_features,\n",
    "            'hunk': collated_hunk_features_batches, # This is now a list of batches, one per original example that had hunks\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Trainer\n",
    "\n",
    "This custom trainer overrides the `compute_loss` method to calculate a weighted loss across the three tasks (prediction, explanation, and hunks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        pred_inputs = inputs.get('pred')\n",
    "        expl_inputs = inputs.get('expl')\n",
    "        hunk_inputs_list = inputs.get('hunk') # This is a list of batches\n",
    "\n",
    "        total_loss = 0\n",
    "        pred_loss = 0\n",
    "        expl_loss = 0\n",
    "        hunk_loss_val = 0 # Initialize to float\n",
    "        num_hunk_batches_processed = 0\n",
    "\n",
    "        # Prediction task loss\n",
    "        if pred_inputs and pred_inputs.get('input_ids').numel() > 0 : # Check if pred_inputs is not empty\n",
    "            # print(f\"Pred input_ids shape: {pred_inputs['input_ids'].shape}\")\n",
    "            # print(f\"Pred labels shape: {pred_inputs['labels'].shape}\")\n",
    "            outputs_pred = model(**pred_inputs)\n",
    "            pred_loss = outputs_pred.loss\n",
    "            total_loss += pred_loss\n",
    "        else:\n",
    "            # print(\"Skipping prediction loss, pred_inputs is empty or invalid.\")\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Explanation task loss\n",
    "        if WEIGHT_BETA > 0 and expl_inputs and expl_inputs.get('input_ids').numel() > 0: # Check if expl_inputs is not empty\n",
    "            # print(f\"Expl input_ids shape: {expl_inputs['input_ids'].shape}\")\n",
    "            # print(f\"Expl labels shape: {expl_inputs['labels'].shape}\")\n",
    "            outputs_expl = model(**expl_inputs)\n",
    "            expl_loss = outputs_expl.loss\n",
    "            total_loss += WEIGHT_BETA * expl_loss\n",
    "        elif WEIGHT_BETA > 0:\n",
    "            # print(\"Skipping explanation loss, expl_inputs is empty or invalid but WEIGHT_BETA > 0.\")\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Hunk task loss - careful here, hunk_inputs_list is a list of batch dictionaries\n",
    "        if WEIGHT_GAMMA > 0 and hunk_inputs_list:\n",
    "            current_hunk_loss_sum = 0\n",
    "            for hunk_batch_inputs in hunk_inputs_list: # Iterate over list of batches\n",
    "                if hunk_batch_inputs and hunk_batch_inputs.get('input_ids').numel() > 0 : # Check if batch is not empty\n",
    "                    # print(f\"Hunk input_ids shape: {hunk_batch_inputs['input_ids'].shape}\")\n",
    "                    # print(f\"Hunk labels shape: {hunk_batch_inputs['labels'].shape}\")\n",
    "                    outputs_hunk = model(**hunk_batch_inputs)\n",
    "                    current_hunk_loss_sum += outputs_hunk.loss\n",
    "                    num_hunk_batches_processed += 1\n",
    "                # else:\n",
    "                    # print(\"Skipping a hunk batch, it's empty or invalid.\")\n",
    "\n",
    "            if num_hunk_batches_processed > 0:\n",
    "                hunk_loss_val = current_hunk_loss_sum / num_hunk_batches_processed # Average loss over hunk batches\n",
    "                total_loss += WEIGHT_GAMMA * hunk_loss_val\n",
    "            # elif num_hunk_batches_processed == 0 and WEIGHT_GAMMA > 0.0 :\n",
    "                # print(\"No hunk batches were processed, though WEIGHT_GAMMA > 0\")\n",
    "\n",
    "\n",
    "        # Log individual losses\n",
    "        self.log({\n",
    "            \"pred_loss\": pred_loss.item() if isinstance(pred_loss, torch.Tensor) else pred_loss,\n",
    "            \"expl_loss\": expl_loss.item() if isinstance(expl_loss, torch.Tensor) else expl_loss,\n",
    "            \"hunk_loss\": hunk_loss_val.item() if isinstance(hunk_loss_val, torch.Tensor) else hunk_loss_val,\n",
    "            \"total_weighted_loss\": total_loss.item() if isinstance(total_loss, torch.Tensor) else total_loss\n",
    "        })\n",
    "\n",
    "        return (total_loss, {\"pred_outputs\": outputs_pred, \"expl_outputs\": outputs_expl}) if return_outputs else total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. LoRA Configuration\n",
    "\n",
    "Set up the LoRA (Low-Rank Adaptation) configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 64,225,280 || all params: 14,832,532,480 || trainable%: 0.4330\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                             # Rank of the LoRA matrices\n",
    "    lora_alpha=32,                    # Alpha parameter for LoRA scaling\n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \n",
    "        \"up_proj\", \n",
    "        \"down_proj\"\n",
    "    ],                                # Modules to apply LoRA to (specific to Qwen architecture)\n",
    "    lora_dropout=0.05,                # Dropout probability for LoRA layers\n",
    "    bias=\"none\",                      # Bias type for LoRA. 'none' is common.\n",
    "    task_type=\"CAUSAL_LM\"             # Task type\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print a summary of the trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Instantiate the custom data collator\n",
    "data_collator = TaskPrefixDataCollator(tokenizer=tokenizer, mlm=False) # mlm=False for Causal LM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Arguments\n",
    "\n",
    "Configure the training arguments. Adjust these based on your available resources and desired training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,                     # Number of training epochs (adjust as needed)\n",
    "    per_device_train_batch_size=1,          # Batch size per GPU (reduce if OOM errors)\n",
    "    gradient_accumulation_steps=1,          # Accumulate gradients over X steps (effective batch size = X * per_device_train_batch_size)\n",
    "    gradient_checkpointing=True,            # Use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",              # Optimizer\n",
    "    save_steps=200,                         # Save checkpoint every X steps\n",
    "    logging_steps=20,                       # Log metrics every X steps\n",
    "    learning_rate=2e-4,                     # Learning rate\n",
    "    weight_decay=0.001,                     # Weight decay\n",
    "    fp16=False,                             # Set to True if your GPU supports FP16 and you want faster training\n",
    "    bf16=True,                              # Set to True if your GPU supports BF16 (e.g., A100, H100)\n",
    "    max_grad_norm=0.3,                      # Max gradient norm for clipping\n",
    "    max_steps=-1,                           # Number of training steps (set to -1 for full epochs)\n",
    "    warmup_ratio=0.03,                      # Warmup ratio for learning rate scheduler\n",
    "    group_by_length=False,                  # Group sequences by length (can improve efficiency)\n",
    "    lr_scheduler_type=\"constant\",           # Learning rate scheduler type\n",
    "    report_to=\"tensorboard\",                # Log to tensorboard\n",
    "    # evaluation_strategy=\"steps\" if eval_dataset else \"no\", # Evaluate periodically if eval_dataset exists\n",
    "    # eval_steps=200 if eval_dataset else None, # Evaluation frequency\n",
    "    save_total_limit=2,                     # Only keep the last 2 checkpoints\n",
    "    load_best_model_at_end=False,           # Whether to load the best model (if evaluating) at the end\n",
    "    remove_unused_columns=False,            # Important for custom collator that expects 'text' or specific structures\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Trainer and Start Training\n",
    "\n",
    "Initialize the custom trainer with the model, datasets, tokenizer, data collator, and training arguments. Then, start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_161101/2524373608.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "Using auto half precision backend\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,535\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4,605\n",
      "  Number of trainable parameters = 64,225,280\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4605' max='4605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4605/4605 2:23:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.972500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.767800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.730100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.550200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.580500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.562000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.652300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.559700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.528200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.659900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.436400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.476500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.542400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.439700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.436600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.523000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.402800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.437900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.485600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.504200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.434200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.439600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.456100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.436200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.429400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.386300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.431300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.430700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.369200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.369100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.378900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.337000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>0.353500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>0.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>0.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>0.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>0.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>0.349100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>0.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>0.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>0.351000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>0.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>0.300500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>0.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>0.302700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>0.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>0.325900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>0.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>0.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>0.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.294700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>0.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>0.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>0.334100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>0.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>0.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>0.233300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>0.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>0.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>0.317800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.308600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>0.368600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>0.297300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>0.334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>0.274200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>0.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>0.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>0.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>0.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>0.327800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>0.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>0.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>0.261700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.323900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>0.312900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>0.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>0.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.215300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>0.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>0.224100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>0.200400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>0.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>0.211300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>0.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>0.206400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>0.222400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.222100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>0.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>0.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>0.187900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>0.263300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>0.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.234900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>0.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>0.208400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>0.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.217900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>0.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>0.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>0.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.279600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>0.231300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>0.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>0.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>0.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>0.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>0.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>0.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>0.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>0.290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>0.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>0.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>0.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>0.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>0.251200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>0.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>0.247700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.230800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>0.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>0.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>0.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to 8111/checkpoint-200\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd6417ca30>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f9f11908-03d0-4721-a136-b5568019b772)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-200/special_tokens_map.json\n",
      "Saving model checkpoint to 8111/checkpoint-400\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda539c9a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: a72a1c89-6f54-45d5-b510-96bb297bcfb7)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-96] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-600\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4d24640>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: eb37dbd2-302a-4742-a12c-dc6b9226a5b9)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-200] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-800\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd641da890>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 2a7ef61a-353d-4167-ac9c-ef0d95e7b1a9)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-400] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-1000\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4ad0430>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 17086657-3c4f-4bd3-aa81-200b93acfbbb)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-600] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-1200\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4b8a1d0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 4bbbd447-48e3-4c9c-a080-467ef8ffd3a2)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-800] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-1400\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda539ca90>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: e4dbf807-09a1-478e-9ad6-911155ff59fd)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-1600\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4e43cd0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 21ef4e48-abda-4e7e-8450-a1a0235549c7)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-1600/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-1600/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-1200] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-1800\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd4a1c73d0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 552b370e-373b-40f8-a9c0-82fa204559d9)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-1800/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-1800/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-1400] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-2000\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4dbf0a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 675a254d-a008-447f-94ec-78786978fc7b)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-1600] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-2200\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd4a1c7ac0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: ed80d348-21ed-4d8a-822b-3c309af47814)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-2200/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-2200/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-1800] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-2400\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda5180040>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 183daf3f-851d-45c9-ab6a-5d2223b93228)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-2400/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-2400/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-2600\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda5506a70>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 3967d7e2-136d-4f4a-acb9-0b279825937e)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-2600/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-2600/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-2200] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-2800\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda539cbb0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 500459ac-3571-420a-ba31-1d2108d33db0)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-2800/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-2800/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-2400] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-3000\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd50544c10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 3baeecdd-3028-4294-bbe4-a979fee87b59)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-2600] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-3200\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda58b93c0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: dce26f13-25af-4288-9ace-cd4f5400e678)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-3200/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-3200/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-2800] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-3400\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda54ae6e0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 48042ce5-c8ff-4d6f-8ebf-6c98feb466ae)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-3400/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-3400/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-3600\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd4a1c5120>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 2b36e3ee-f207-4493-b17e-750d0aad8a3b)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-3600/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-3600/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-3200] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-3800\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd4a1c6fb0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f84303f3-8f7a-4d19-a530-53860fff9fe3)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-3800/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-3800/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-3400] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-4000\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda51804c0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 554ce743-f7db-4d59-bee5-692f1922f7cf)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-3600] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-4200\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4e1a6e0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: ce8b623f-40d8-4cca-9822-94c6ab3cbfdb)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-4200/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-4200/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-3800] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-4400\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd4bb76f80>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 06143771-d71a-45cd-ab08-eaece354b625)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-4400/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-4400/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-4600\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda57af460>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 108e7df1-4f41-409f-8156-15fd33c22ea4)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-4600/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-4600/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-4200] due to args.save_total_limit\n",
      "Saving model checkpoint to 8111/checkpoint-4605\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdda4f622c0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 9ae4233d-4542-4ad9-9f53-b244fee2f49e)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/checkpoint-4605/tokenizer_config.json\n",
      "Special tokens file saved in 8111/checkpoint-4605/special_tokens_map.json\n",
      "Deleting older checkpoint [8111/checkpoint-4400] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to 8111/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/other.py:1110: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen3-14B/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fdd644957e0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: b22194a3-20cb-4e96-a3b6-7ca231718aa0)') - silently ignoring the lookup for the file config.json in Qwen/Qwen3-14B.\n",
      "  warnings.warn(\n",
      "/home/barty/research/morepair-exp/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:236: UserWarning: Could not find a config file in Qwen/Qwen3-14B - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "tokenizer config file saved in 8111/tokenizer_config.json\n",
      "Special tokens file saved in 8111/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved to 8111/\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # Will be None if not created\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(OUTPUT_DIR) # Save the LoRA adapter\n",
    "# model.save_pretrained(OUTPUT_DIR) # Alternative way to save PEFT model\n",
    "\n",
    "# If you want to save the full model (merged with base model), you can do the following:\n",
    "# Ensure the base_model is not quantized or on CPU for merging if issues arise.\n",
    "# merged_model = model.merge_and_unload()\n",
    "# merged_model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\"))\n",
    "# tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\"))\n",
    "\n",
    "print(f\"Training complete. Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# Clean up GPU memory (optional, but good practice in Colab)\n",
    "del model\n",
    "del base_model\n",
    "del trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference with the Fine-tuned Model\n",
    "\n",
    "This section demonstrates how to load the fine-tuned LoRA adapters and use the model for inference.\n",
    "\n",
    "**Note:**\n",
    "* If you saved the full merged model, you would load it directly using `AutoModelForCausalLM.from_pretrained(\"YOUR_OUTPUT_DIR/final_merged_checkpoint\")` and `AutoTokenizer.from_pretrained(\"YOUR_OUTPUT_DIR/final_merged_checkpoint\")`.\n",
    "* For LoRA, we load the base model and then apply the saved adapters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: Qwen/Qwen3-14B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 5120,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 17408,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 40,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 40,\n",
      "  \"num_hidden_layers\": 40,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/model.safetensors.index.json\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:09<00:00,  1.15s/it]\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-14B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for: Qwen/Qwen3-14B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/vocab.json\n",
      "loading file merges.txt from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/merges.txt\n",
      "loading file tokenizer.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/barty/.cache/huggingface/hub/models--Qwen--Qwen3-14B/snapshots/231c69a380487f6c0e52d02dcf0d5456d1918201/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from: 8111/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model ready.\n",
      "Creating pipeline on device: cuda:0\n",
      "Pipeline created.\n",
      "\n",
      "--- Generating repair (humaneval-cpp.py style) ---\n",
      "Attempt 1/2...\n",
      "Success: Extracted code segment (attempt 1).\n",
      "\n",
      "--- Final Repaired Code ---\n",
      "#include<stdio.h>\n",
      "#include<vector>\n",
      "using namespace std;\n",
      "int add_elements(vector<int> arr,int k){\n",
      "\n",
      "\n",
      "    int sum=0;\n",
      "    for (int i=0;i<k;i++)\n",
      "        if( arr[i]<=-99 or arr[i]>=99)\n",
      "            sum-=arr[i];\n",
      "    return sum;\n",
      "}\n",
      "\n",
      "--- Diff showing only content changes ---\n",
      "--- original_buggy.cpp\\n+++ repaired_generated.cpp\\n@@ -2,9 +2,11 @@\\n #include<vector>\n",
      " using namespace std;\n",
      " int add_elements(vector<int> arr,int k){\n",
      "     int sum=0;\n",
      "     for (int i=0;i<k;i++)\n",
      "-        if( arr[i]<=-99 and arr[i]>=99)\n",
      "+        if( arr[i]<=-99 or arr[i]>=99)\n",
      "             sum-=arr[i];\n",
      "     return sum;\n",
      "-}\n",
      "+}"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import gc\n",
    "import re\n",
    "from transformers import pipeline\n",
    "import difflib # Import difflib to calculate differences\n",
    "\n",
    "# --- Configuration ---\n",
    "PEFT_MODEL_PATH = OUTPUT_DIR # Assumes OUTPUT_DIR is defined in a previous cell\n",
    "\n",
    "# --- Reload Model & Tokenizer ---\n",
    "if 'MODEL_NAME' not in globals(): MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "if 'OUTPUT_DIR' not in globals(): OUTPUT_DIR = \"8111/\"; PEFT_MODEL_PATH = OUTPUT_DIR\n",
    "\n",
    "if 'bnb_config' not in globals():\n",
    "    print(\"Re-defining bnb_config...\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "inference_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME, quantization_config=bnb_config, torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Loading tokenizer for: {MODEL_NAME}\")\n",
    "inference_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "inference_tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(f\"Loading LoRA adapters from: {PEFT_MODEL_PATH}\")\n",
    "inference_model = PeftModel.from_pretrained(inference_base_model, PEFT_MODEL_PATH)\n",
    "inference_model = inference_model.eval()\n",
    "print(\"Fine-tuned model ready.\")\n",
    "\n",
    "# --- humaneval-cpp.py Style Inference Setup ---\n",
    "BOF = '<|system|><|im_end|><|user|>'\n",
    "EOF = '<|im_end|><|assistant|>'\n",
    "\n",
    "def extract_first_cpp_code(text_to_search_in):\n",
    "    match = re.search(r\"```cpp\\n(.*?)\\n```\", text_to_search_in, re.DOTALL)\n",
    "    if match: return match.group(1).strip()\n",
    "    match = re.search(r\"```c\\+\\+\\n(.*?)\\n```\", text_to_search_in, re.DOTALL)\n",
    "    if match: return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "print(f\"Creating pipeline on device: {inference_model.device}\")\n",
    "pipe = pipeline(\"text-generation\", model=inference_model, tokenizer=inference_tokenizer)\n",
    "print(\"Pipeline created.\")\n",
    "\n",
    "# --- Data from humaneval-cpp/buggy/add_elements.cpp ---\n",
    "_problem_description_text = \"\"\"Given a non-empty vector of integers arr and an integer k, return\n",
    "the sum of the elements with at most two digits from the first k elements of arr.\n",
    "Example:\n",
    "    Input: arr = {111,21,3,4000,5,6,7,8,9}, k = 4\n",
    "    Output: 24 # sum of 21 + 3\n",
    "Constraints:\n",
    "    1. 1 <= len(arr) <= 100\n",
    "    2. 1 <= k <= len(arr)\n",
    "\"\"\"\n",
    "_incorrect_code_text = \"\"\"#include<stdio.h>\n",
    "#include<vector>\n",
    "using namespace std;\n",
    "int add_elements(vector<int> arr,int k){\n",
    "    int sum=0;\n",
    "    for (int i=0;i<k;i++)\n",
    "        if( arr[i]<=-99 and arr[i]>=99)\n",
    "            sum-=arr[i];\n",
    "    return sum;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def generate_repaired_code_via_pipeline(problem_desc, incorrect_code, tokenizer_for_pipe, pipe_instance):\n",
    "    filename_placeholder = \"add_elements.cpp\"\n",
    "    prompt_suffix_for_assistant = \"\"\"#include<stdio.h>\n",
    "#include<vector>\n",
    "using namespace std;\n",
    "int add_elements(vector<int> arr,int k){\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    prompt = (BOF +\n",
    "              f\" This is an incorrect code ({filename_placeholder}):\\n```c++\\n{incorrect_code}\\n```\\n\" +\n",
    "              \"You are a software engineer. Can you repair the incorrect code?\\n\" +\n",
    "              EOF + \"\\n```c++\\n\" + prompt_suffix_for_assistant)\n",
    "\n",
    "    print(f\"\\n--- Generating repair (humaneval-cpp.py style) ---\")\n",
    "    prompt_token_count = len(tokenizer_for_pipe.tokenize(prompt))\n",
    "    min_new_tokens = 64\n",
    "    max_new_tokens = 512\n",
    "    max_attempts = 2\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        print(f\"Attempt {attempt + 1}/{max_attempts}...\")\n",
    "        original_padding_side = tokenizer_for_pipe.padding_side\n",
    "        if tokenizer_for_pipe.padding_side != \"left\": tokenizer_for_pipe.padding_side = \"left\"\n",
    "\n",
    "        outputs = pipe_instance(\n",
    "            prompt, min_length=prompt_token_count + min_new_tokens,\n",
    "            max_length=prompt_token_count + max_new_tokens,\n",
    "            temperature=0.3, do_sample=True, num_return_sequences=1,\n",
    "            pad_token_id=tokenizer_for_pipe.eos_token_id,\n",
    "            eos_token_id=tokenizer_for_pipe.eos_token_id\n",
    "        )\n",
    "        if tokenizer_for_pipe.padding_side != original_padding_side: tokenizer_for_pipe.padding_side = original_padding_side\n",
    "\n",
    "        full_generated_text = outputs[0]['generated_text']\n",
    "        parts_after_eof = full_generated_text.split(EOF, 1)\n",
    "\n",
    "        if len(parts_after_eof) > 1:\n",
    "            assistant_response = parts_after_eof[1].strip()\n",
    "            repaired_code_segment = extract_first_cpp_code(assistant_response)\n",
    "            if repaired_code_segment is not None:\n",
    "                print(f\"Success: Extracted code segment (attempt {attempt+1}).\")\n",
    "                return repaired_code_segment\n",
    "            else:\n",
    "                 print(f\"Failed: Could not extract ```cpp block (attempt {attempt+1}).\")\n",
    "        else:\n",
    "            print(f\"Failed: Could not find EOF marker (attempt {attempt+1}).\")\n",
    "        print(f\"Retrying if possible...\")\n",
    "\n",
    "    print(\"Failed after multiple attempts.\")\n",
    "    return \"// Error: Could not generate repair.\"\n",
    "\n",
    "# --- Execute Inference ---\n",
    "repaired_code_result = generate_repaired_code_via_pipeline(\n",
    "    _problem_description_text, _incorrect_code_text, inference_tokenizer, pipe\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Final Repaired Code ---\")\n",
    "print(repaired_code_result)\n",
    "\n",
    "# --- Calculate and Print Diff (Filtered) ---\n",
    "if not repaired_code_result.startswith(\"// Error\"):\n",
    "    print(f\"\\n--- Diff showing only content changes ---\")\n",
    "    original_lines = _incorrect_code_text.splitlines(keepends=True)\n",
    "    repaired_lines = repaired_code_result.splitlines(keepends=True)\n",
    "\n",
    "    diff = difflib.unified_diff(\n",
    "        original_lines, repaired_lines,\n",
    "        fromfile='original_buggy.cpp', tofile='repaired_generated.cpp',\n",
    "        lineterm='\\\\n'\n",
    "    )\n",
    "\n",
    "    # Print the filtered diff lines\n",
    "    diff_output_exists = False\n",
    "    for line in diff:\n",
    "        # Only print headers, context markers, common lines,\n",
    "        # or add/delete lines that contain non-whitespace characters\n",
    "        if line.startswith(('---', '+++', '@@', ' ')):\n",
    "            print(line, end='')\n",
    "            diff_output_exists = True\n",
    "        elif line.startswith(('-', '+')) and line[1:].strip(): # Check if content exists after +/-\n",
    "            print(line, end='')\n",
    "            diff_output_exists = True\n",
    "        # Else: skip printing lines starting with +/- followed only by whitespace\n",
    "\n",
    "    if not diff_output_exists:\n",
    "         print(\"(No significant content differences found, only whitespace changes)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Diff not calculated due to generation error ---\")\n",
    "\n",
    "# Optional: Clean up\n",
    "# del inference_model, inference_base_model, pipe; gc.collect(); torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
